---
title: "homework5_Jiaxin"
output: html_document
---

---
title: "Homework 5"
author: "Brandon Chung, Jiaxin Zheng, Andreina Arias, and Stephanie Chiang"
date: "Fall 2025"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
---

## Overview

In this homework assignment, you will explore, analyze and model a data set containing information on
approximately 12,000 commercially available wines. The variables are mostly related to the chemical properties of the wine being sold. The response variable is the number of sample cases of wine that were purchased by wine
distribution companies after sampling a wine. These cases would be used to provide tasting samples to
restaurants and wine stores around the United States. The more sample cases purchased, the more likely is a
wine to be sold at a high end restaurant. A large wine manufacturer is studying the data in order to predict the
number of wine cases ordered based upon the wine characteristics. If the wine manufacturer can predict the
number of cases, then that manufacturer will be able to adjust their wine offering to maximize sales.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(mice)
library(corrplot)
library(dplyr)
library(e1071)
library(ggplot2)
library(gridExtra)
library(caret)
library(pROC)
library(kableExtra)
library(tinytex)
library(tidyr)
library(patchwork)
library(Metrics)
library(knitr)
```


## 1. DATA EXPLORATION

Explore, analyze and model a data set containing approximately 8000 records representing a customer at an auto insurance company. Each record has two response variables. The first response variable, TARGET_FLAG, is a 1 or a 0. A “1” means that the person was in a car crash. A zero means that the person was not in a car crash. The second response variable is TARGET_AMT. This value is zero if the person did not crash their car. But if they did crash their car, this number will be a value greater than zero.

VARIABLE NAME DEFINITION THEORETICAL EFFECT

- INDEX: Identification Variable (do not use) None
- TARGET: Number of Cases Purchased None
- AcidIndex: Proprietary method of testing total acidity of wine by using a weighted average
- Alcohol: Alcohol Content
- Chlorides: Chloride content of wine
- CitricAcid: Citric Acid Content
- Density: Density of Wine
- FixedAcidity: Fixed Acidity of Wine
- FreeSulfurDioxide: Sulfur Dioxide content of wine
- LabelAppeal: Marketing Score indicating the appeal of label design for consumers. High numbers
suggest customers like the label design. Negative numbers suggest customes
don't like the design. (Many consumers purchase based on the visual appeal of the
wine label design. Higher numbers suggest better sales.)
- ResidualSugar: Residual Sugar of wine
- STARS: Wine rating by a team of experts. 4 Stars = Excellent, 1 Star = Poor (A high number of stars suggests high sales)
- Sulphates: Sulfate conten of wine
- TotalSulfurDioxide: Total Sulfur Dioxide of Wine
- VolatileAcidity: Volatile Acid content of wine
- pH: pH of wine

#### Dataset

The dataset contains 16 columns and 12,795 rows. An initial review shows a number of missing values, with 8 columns containing NAs. We also observe that several numerical features include negative values and no columns with near zero variance.

```{r import}
train<- read.csv("https://raw.githubusercontent.com/Chung-Brandon/621-Data-Mining/refs/heads/main/wine-training-data.csv")

test <- read.csv("https://raw.githubusercontent.com/Chung-Brandon/621-Data-Mining/refs/heads/main/wine-evaluation-data.csv")
```

```{r}
glimpse(train)
```

```{r}
summary(train)
```
```{r}
colSums(is.na(train))
```

```{r echo=FALSE}
# Drop the INDEX column
train <- train %>% 
  dplyr::select(-INDEX)

test <- test %>% 
  dplyr::select(-IN)
```

```{r}
nzv <- nearZeroVar(train, saveMetrics = TRUE)
nzv$nzv
```


#### Distributions

- We see that most variables have normal distribution, like Alcohol, FixedAcidity, Density, pH, Sulphates, VolatileAcidity have strong central peaks. 

- FreeSulfurDioxide and TotalSulfurDioxide extend from negative to positive values. This indicates data quality issues or special cases that may require transformation.

- Variable AcidIndex, STARS, and LabelAppeal shows a multimodal distribution.

```{r}
train_long <- train %>% 
  pivot_longer(
    cols = everything(),
    names_to  = "variable",
    values_to = "value"
  )

ggplot(train_long, aes(x = value)) +
  geom_histogram(aes(y = ..density..),
                 bins = 30,
                 fill = "grey40") +
  geom_density(color = "blue", linewidth = 0.7, na.rm = TRUE) +
  facet_wrap(~ variable, scales = "free", ncol = 4) +
  labs(x = NULL, y = "density", title = "Distributions of Wine Variables") +
  theme_minimal()

```



#### Boxplots

- Several variables have extreme outliers. 
- ResidualSugar, TotalSulfurDioxide, FreeSulfurDioxide have very large positive and negative outliers.

```{r}
# Boxplots
ggplot(train_long, aes(x = variable, y = value)) +
  geom_boxplot(outlier.color = "red", outlier.size = 0.8, na.rm = TRUE) +
  facet_wrap(~ variable, scales = "free", ncol = 4) +
  labs(
    title = "Boxplots of Wine Variables"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_blank(),
    strip.text = element_text(size = 10, face = "bold")
  )
```

#### Correlation matrix

From our correlation matrix we can see that Acidindex seems to have the greatest impact on our TARGET, the number of cases purchased, this finding will warrent further investigation. Also, we can note here that other numeric variables do not seem to have large impacts on the TARGET variable at first glance.  

```{r}
# Correlation matrix for the numeric variables, excluding STARS and LabelAppeal as we are treating them as categorical variables

corr_matrix <- cor(train[, c("TARGET","FixedAcidity", "VolatileAcidity", "CitricAcid","ResidualSugar","Chlorides","FreeSulfurDioxide","TotalSulfurDioxide","Density","pH","Sulphates","Alcohol","AcidIndex")], use = "complete.obs")

corrplot(
  corr_matrix,
  method = "color",
  type = "upper", 
  order = "hclust",
  addCoef.col = "black", 
  tl.col = "black", 
  tl.srt = 45, 
  number.cex = 0.6,
  col = colorRampPalette(c("darkred", "white", "darkblue"))(200)
)
```

## 2. DATA Preparation

#### Missing Values

For the missing values in the STARS variable, this implicates that the wine is unranked. To capture the unranked status we have imputated NA values in STARS as "unranked". MICE imputation here would imply rankings for the wine where rankings may not be appropriate.

```{r}

train <- train %>%
  mutate(STARS = ifelse(is.na(STARS), "unranked", STARS),
         STARS = as.factor(STARS))

train <- train %>%
  mutate(LabelAppeal = as.factor(LabelAppeal))
```

We use mice package to impute the remaining missing values in the dataset.

```{r}
# Impute missing data using mice

set.seed(123)
imp <- mice(train, m = 5, method = "pmm")
train_mice <- complete(imp)

```
```{r}
colSums(is.na(train_mice))
```

Corrections were also applied to the test data
```{r}

test <- test %>%
  mutate(STARS = ifelse(is.na(STARS), "unranked", STARS),
         STARS = as.factor(STARS))

test <- test %>%
  mutate(LabelAppeal = as.factor(LabelAppeal))
```

The testing data also had missing data used mice a well to impute missing data without imputing data to the target variables. The target variables were turned to empty strings to avoid mice from filling in those variables.
```{r}
# specify methods for each column
methVar <- make.method(test)

# set target columns to empty strings "" so mice will not touch them
methVar[methVar != ""] <- "pmm"
methVar["TARGET"] <- ""

imp <- (mice(test, m = 5, method = methVar, seed = 123))
test_imp <- complete(imp)


colSums(is.na(test_imp))
```



#### Outliers

Many numerical features have somewhat unreasonable negative values, we decided to shift the variables that had negative values to have the lowest values just above 0. 

```{r}
shift_to_positive <- function(x, eps = 0.01){
  x - min(x, na.rm = TRUE) + eps
}
```


```{r}
chem_vars <- c(
  "FixedAcidity",
  "VolatileAcidity",
  "CitricAcid",
  "ResidualSugar",
  "Chlorides",
  "FreeSulfurDioxide",
  "TotalSulfurDioxide",
  "Sulphates", 
  "Alcohol"
)
```


```{r}
train_mice[chem_vars] <- lapply(train_mice[chem_vars], shift_to_positive)
test_imp[chem_vars]   <- lapply(test_imp[chem_vars], shift_to_positive)
```


Density plot to recheck distribution of numeric variables.
```{r}
# Select numeric variables only (density requires numeric)
num_vars <- names(train_mice)[sapply(train_mice, is.numeric)]

plot_list <- lapply(num_vars, function(var) {
  ggplot(train_mice, aes(x = .data[[var]])) +
    geom_histogram(aes(y = ..density..),
                   bins = 30,
                   fill = "grey40") +
    geom_density(color = "blue", linewidth = 0.7, na.rm = TRUE) +
    labs(title = var, x = NULL, y = "density") +
    theme_minimal()
})

wrap_plots(plot_list, ncol = 4)
```

### Models

Poisson Regression Model 1- baseline with all variables.
```{r}
Model_p1<-glm(TARGET~., data=train_mice, family=poisson)
summary(Model_p1)
```
Poisson Regression Model 2- with chemical variables.
```{r}
set.seed(123)

Model_p2<- glm(TARGET ~ .-FixedAcidity -VolatileAcidity -CitricAcid -ResidualSugar -Chlorides -FreeSulfurDioxide -TotalSulfurDioxide -Sulphates -Alcohol, data = train_mice, family=poisson)
summary(Model_p2)
```

Multiple linear regression Model- baseline with all variables
```{r}
Model_M<-lm(TARGET~.,data = train_mice)
summary(Model_M)
```

Multiple Linear regression with chemical variables

```{r}
set.seed(123)

Model_M2<- lm(TARGET ~ .-FixedAcidity -VolatileAcidity -CitricAcid -ResidualSugar -Chlorides -FreeSulfurDioxide -TotalSulfurDioxide -Sulphates -Alcohol, data = train_mice)
summary(Model_M2)
```

### Predictions

```{r}
train_pred1 <- predict(Model_p1, newdata = train_mice)
test_pred1<- predict(Model_p1, newdata = test_imp)
```



```{r}
train_pred2 <- predict(Model_p2, newdata = train_mice)

test_pred2 <- predict(Model_p2, newdata = test_imp)
```


```{r}
train_pred3 <- predict(Model_M, newdata = train_mice)

test_pred3 <- predict(Model_M, newdata = test_imp)
```



```{r}
train_pred4 <- predict(Model_M2, newdata = train_mice)

test_pred4 <- predict(Model_M2, newdata = test_imp)
```

## Comparsion of Models
```{r}
# RMSE & MAE
rmse_p1 <- rmse(train_mice$TARGET, train_pred1)
mae_p1  <- mae(train_mice$TARGET, train_pred1)

rmse_p2 <- rmse(train_mice$TARGET, train_pred2)
mae_p2  <- mae(train_mice$TARGET, train_pred2)

rmse_m3 <- rmse(train_mice$TARGET, train_pred3)
mae_m3  <- mae(train_mice$TARGET, train_pred3)

rmse_m4 <- rmse(train_mice$TARGET, train_pred4)
mae_m4  <- mae(train_mice$TARGET, train_pred4)
```

```{r}
# AIC
aic_p1 <- AIC(Model_p1)
aic_p2 <- AIC(Model_p2)
aic_m3 <- AIC(Model_M)
aic_m4 <- AIC(Model_M2)
```


```{r}
# R-squared
# For linear regression
rsq_m3 <- summary(Model_M)$r.squared
rsq_m4 <- summary(Model_M2)$r.squared

# For Poisson regression (pseudo R²)
rsq_p1 <- 1 - (Model_p1$deviance / Model_p1$null.deviance)
rsq_p2 <- 1 - (Model_p2$deviance / Model_p2$null.deviance)

```



```{r}
model_metrics <- data.frame(
  Model = c("Poisson Regression",
            "Poisson Regression model 2",
            "Linear Regression",
            "Linear Regression Model 2"),
  RMSE  = c(rmse_p1, rmse_p2, rmse_m3, rmse_m4),
  MAE   = c(mae_p1, mae_p2, mae_m3, mae_m4),
  AIC   = c(aic_p1, aic_p2, aic_m3, aic_m4),
  R_squared = c(rsq_p1, rsq_p2, rsq_m3, rsq_m4)
)


kable(model_metrics, caption = "Comparison of Regression Models with RMSE, MAE, AIC, and R-squared") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F)
```




